{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f55f9f30-e5f7-4d9f-89e2-cc6e555d123f",
   "metadata": {},
   "source": [
    "# Text Classificaiton: BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c088b7c-39f7-451f-ac6b-b9da859b85b5",
   "metadata": {},
   "source": [
    "**텍스트 분류(Text classificaiton)** 는 입력 테스트를 미리 정의된 범주나 레이블로 할당하는 과제를 의미한다.\n",
    "\n",
    "**BERT(Bidirectional Encoder Representations form Transformers)** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de36ca-3dcb-42dd-9f64-81c419c6595b",
   "metadata": {},
   "source": [
    "## BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7002af-65d7-4bbb-9ee4-85b367f21bf6",
   "metadata": {},
   "source": [
    "BERT는 워드피스 토크나이저를 사용한다.\n",
    "- 워드피스 : 단어를 더 작은 서브워드 단위로 나누는 방식\n",
    "    - OOV 문제 완화\n",
    "    - 데이터 기반으로 토큰 집합 생성 -> 도메인 적응성 향상"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a28149c-594e-4f20-9be1-97d11e06e1f0",
   "metadata": {},
   "source": [
    "**Tokenization using BERT Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "755c71ea-127d-40e2-8223-c6e3c22b7c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78359ecf-77dc-406f-a565-e938f2f67397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a167cc8e3e64e0c829b59fe32d40974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc8f9a434574be89292e105e5c9ad86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4960cf4ca5214377bb3c7b6dedca79df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.72M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03ac5eb974b431381ead29c8a87f4d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c5b022d-1ddc-4bf4-8954-6c059699d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Transformers Is so COOL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd3f7ade-b17b-495d-a521-904b02f319e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 58263, 10127, 10297, 26462, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer(text)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3b9ad1-2114-4a5a-8a40-47097422f6ae",
   "metadata": {},
   "source": [
    "- `input_ids` : 입력 텍스트를 정수 인코딩으로 변환한 값\n",
    "- `token_type_ids` : 입력이 여러 세그먼트로 구성된 경우 각 세그먼트를 구분하는 값\n",
    "- `attention_mask` : 트랜스포머 인코더의 셀프 어텐션에 사용되는 마스크 값, 모델이 어떤 토큰을 무시해야 하는지를 지정하는 역할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e365bcec-e00d-4e23-bc5c-00f1301aeeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 23:27:47.670046: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-03 23:27:47.772344: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-03 23:27:47.776119: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-11-03 23:27:47.776129: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-11-03 23:27:47.795118: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-03 23:27:48.378190: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-11-03 23:27:48.378238: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-11-03 23:27:48.378244: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] transformers is so cool [SEP]\n"
     ]
    }
   ],
   "source": [
    "input_ids = encoded[\"input_ids\"]\n",
    "decoded = tokenizer.decode(input_ids)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780f5aaa-397a-40af-b0ac-f122a2bb83d0",
   "metadata": {},
   "source": [
    "- 전처리 단계에서 모든 문자를 소문자로 변환\n",
    "    - 동일한 단어가 대소문자로 인해 다르게 표기되는 문제 해결 -> 데이터 일관성 확보\n",
    "    - 대소문자를 구분하지 않으므로 어휘 사전의 크기 감소 -> 계산 효율성 향상\n",
    "    - 개체명 인식 등 대소문자 구분이 중요한 과제에서는 성능이 떨어질 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e63a2e8-2524-436c-aab8-aea9ef216c98",
   "metadata": {},
   "source": [
    "## BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cefc062-7243-4f2b-9c44-8c1eb14ee860",
   "metadata": {},
   "source": [
    "1. 임베딩 계층\n",
    "    - 입력 텍스트를 벡터 형태로 변환하는 역할\n",
    "2. 인코더 계층\n",
    "    - 12개의 트랜스포머 인코더 계층으로 구성\n",
    "3. 풀러 계층\n",
    "    - 인코더 계층의 최종 출력을 받아 [CLS] 토큰의 벡터를 추출하고 이를 요약벡터로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56f98b7-070f-4f6c-a7b5-85868f42888f",
   "metadata": {},
   "source": [
    "**Structure of BERT model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23e78cef-73ad-48f0-b502-47622e6895a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06ed25fd-17ff-4fad-8098-890c50ecdf2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e72132000946d8bfd416c5a80f7d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/672M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\"google-bert/bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd47b1a2-8d50-467a-bc1f-4569d8ebd1e6",
   "metadata": {},
   "source": [
    "## Train Text Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcadd3b-1092-4e1f-aaac-6e2acc8492b8",
   "metadata": {},
   "source": [
    "**Tokenizing a Movie Review Sentiment Analysis Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70b91d94-1036-490a-8acf-df8d697089c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8760e01-53a8-48aa-9d95-7df954df2304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(example, tokenizer):\n",
    "    return tokenizer(example[\"document\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99b80cfb-4359-4d1c-b06f-a006882c9d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google-bert/bert-base-multilingual-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ca4bfc5-a212-4f8d-a2e3-81427877ac6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ce9658561a4aac969e83455e62054f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92816fe38934435aa04d57612d1c29be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/3.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911dec6edfdd41f9933d692642df07b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.33M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d44832670554b99a8c14ddaf818be73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.89M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3bfd10257d04cebbae0cd58ada195eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/150000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816a67d1b77349f5adcd25b5f43be7b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"nsmc\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6027f9ad-b947-4c56-bd54-8b47a63eff9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 150000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43685d28-629f-457c-8567-e538cf28f39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아 더빙.. 진짜 짜증나네요 목소리'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d790fbfe-50c7-4639-a021-67799e03e405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ba5cbddd6444988ed9d0026b4779b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4688e625b8e2489498a28250b2667ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_dataset = dataset.map(\n",
    "    lambda example: preprocess_data(example, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=[\"id\", \"document\"]\n",
    ").rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25d3725d-3105-460c-9fcc-d04b4140d145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n",
      "---------------------------------------------------\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n",
      "---------------------------------------------------\n",
      "{'id': '9976970', 'document': '아 더빙.. 진짜 짜증나네요 목소리', 'label': 0}\n",
      "---------------------------------------------------\n",
      "{'labels': 0, 'input_ids': [101, 1174, 25539, 23236, 29234, 13045, 119, 119, 87550, 97082, 25539, 1176, 25539, 24937, 13045, 16801, 72197, 47024, 1169, 70724, 22585, 13926, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(processed_dataset)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(dataset['train'][0])\n",
    "print(\"---------------------------------------------------\")\n",
    "print(processed_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99f2633-acb5-4493-b754-a4e459cd99c9",
   "metadata": {},
   "source": [
    "**DataCollatorWithPadding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40f10532-9d2a-42f6-9d12-b80a401fa25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6295213-4405-43b7-b409-7bc30f9777fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=\"max_length\"\n",
    ")\n",
    "\n",
    "max_length_dataloader = DataLoader(\n",
    "    processed_dataset[\"train\"],\n",
    "    collate_fn=max_length_collator,\n",
    "    batch_size=4,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c1df8ff-9999-44c4-93c5-549b966f0d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length padding input id shape : torch.Size([4, 512])\n"
     ]
    }
   ],
   "source": [
    "max_length_iterator = iter(max_length_dataloader)\n",
    "max_length_batch = next(max_length_iterator)\n",
    "print(\"max_length padding input id shape :\", max_length_batch[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a0f703d-4b17-446c-b2ae-fed36ac8da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=\"longest\"\n",
    ")\n",
    "\n",
    "longest_dataloader = DataLoader(\n",
    "    processed_dataset[\"train\"],\n",
    "    collate_fn=longest_collator,\n",
    "    batch_size=4,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da0eeadd-2378-444a-a02f-c8edae3c9005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longest padding input id shape : torch.Size([4, 42])\n"
     ]
    }
   ],
   "source": [
    "longest_iterator = iter(longest_dataloader)\n",
    "longest_batch = next(longest_iterator)\n",
    "print(\"longest padding input id shape :\", longest_batch[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e271c8d5-1eb2-4d20-bae8-9dc532fbbaa8",
   "metadata": {},
   "source": [
    "**Train text classification model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20d48dda-fb5c-4bcf-bf32-223f344eefb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4695e645-723e-458b-8a48-923302d009ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"text-classification\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=1,\n",
    "    eval_steps=200,\n",
    "    logging_steps=200,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8365f76c-2f41-4fcb-ae93-a0cdaece6555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset[\"train\"].select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ec1834a-1861-4ad7-bfcf-a1c52b24aa5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 00:35:06] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 00:35:06] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 00:35:06] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 00:35:06] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 00:35:06] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 00:35:07] We saw that you have a Intel(R) Core(TM) i7-14700K but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 00:35:07] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-14700K\n",
      "[codecarbon INFO @ 00:35:07] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 00:35:07]   Platform system: Linux-6.8.0-47-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 00:35:07]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 00:35:07]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 00:35:07]   Available RAM : 62.506 GB\n",
      "[codecarbon INFO @ 00:35:07]   CPU count: 28\n",
      "[codecarbon INFO @ 00:35:07]   CPU model: Intel(R) Core(TM) i7-14700K\n",
      "[codecarbon INFO @ 00:35:07]   GPU count: 1\n",
      "[codecarbon INFO @ 00:35:07]   GPU model: 1 x NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=longest_collator,\n",
    "    train_dataset=processed_dataset[\"train\"].select(range(10000)),\n",
    "    eval_dataset=processed_dataset[\"test\"].select(range(100))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ccbd30e-8cc2-498c-90a3-4fecb3f52b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnoahyun1222\u001b[0m (\u001b[33mjiyun\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/user/kjy/DL-projects/hugging_face/003_NLP/wandb/run-20241104_003527-h53mou1k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jiyun/huggingface/runs/h53mou1k' target=\"_blank\">text-classification</a></strong> to <a href='https://wandb.ai/jiyun/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jiyun/huggingface' target=\"_blank\">https://wandb.ai/jiyun/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jiyun/huggingface/runs/h53mou1k' target=\"_blank\">https://wandb.ai/jiyun/huggingface/runs/h53mou1k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 01:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.696800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.696100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.699200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.696800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.695200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.695200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 00:35:43] Energy consumed for RAM : 0.000098 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:35:43] Energy consumed for all GPUs : 0.000809 kWh. Total GPU Power : 193.90044967764211 W\n",
      "[codecarbon INFO @ 00:35:43] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:35:43] 0.001084 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:35:58] Energy consumed for RAM : 0.000195 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:35:58] Energy consumed for all GPUs : 0.001558 kWh. Total GPU Power : 179.7001997738787 W\n",
      "[codecarbon INFO @ 00:35:58] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:35:58] 0.002108 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:36:13] Energy consumed for RAM : 0.000293 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:36:13] Energy consumed for all GPUs : 0.002380 kWh. Total GPU Power : 197.41208047285434 W\n",
      "[codecarbon INFO @ 00:36:13] Energy consumed for all CPUs : 0.000532 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:36:13] 0.003205 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:36:28] Energy consumed for RAM : 0.000391 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:36:28] Energy consumed for all GPUs : 0.003133 kWh. Total GPU Power : 181.04635994122862 W\n",
      "[codecarbon INFO @ 00:36:28] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:36:28] 0.004232 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:36:41] Energy consumed for RAM : 0.000476 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:36:41] Energy consumed for all GPUs : 0.003834 kWh. Total GPU Power : 192.804122044946 W\n",
      "[codecarbon INFO @ 00:36:41] Energy consumed for all CPUs : 0.000863 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:36:41] 0.005173 kWh of electricity used since the beginning.\n",
      "/usr/local/lib/python3.10/dist-packages/codecarbon/output.py:168: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame.from_records([dict(data.values)])])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=0.6963639434814453, metrics={'train_runtime': 75.0946, 'train_samples_per_second': 133.165, 'train_steps_per_second': 16.646, 'total_flos': 416739133918560.0, 'train_loss': 0.6963639434814453, 'epoch': 1.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05bd02b-ff06-4b8a-afb8-fa207f66b470",
   "metadata": {},
   "source": [
    "**Inference text classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e365900-a57e-498d-8e02-9dc773cfc25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6e6a6cf-6001-4da7-95de-06d8f939a19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3126d709-f724-4503-a368-2989497ec984",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"진짜 재밌었어요. 또 보러 갈거에요\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10fca197-6637-4c1d-83d8-f25161f41bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 87550, 97082, 25539,  1175, 26179, 22699, 97104, 13413, 97104,\n",
       "         13413, 47024,   119, 35848,  1170, 29347, 41616, 20966, 12397, 40815,\n",
       "         10609, 47024,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "563f09b0-5081-48d2-9ff7-78c365cbde74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[0.0173, 0.0291]], device='cuda:0'), hidden_states=None, attentions=None)\n",
      "tensor([[0.0173, 0.0291]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs.to(device))\n",
    "    print(outputs)\n",
    "    print(outputs.logits)\n",
    "    print(outputs.logits.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153c9a29-26e3-41d9-8d03-4f7bb3455634",
   "metadata": {},
   "source": [
    "**Evaluate text classification model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "24b40d33-56b0-4d76-89cb-575ce8156275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1c55359f-b8c9-43f1-a153-2403b41c6ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yhat = trainer.predict(processed_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d0e4aa64-7a13-4261-bebc-ef1dedb7b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = yhat.predictions.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "64a1f8ae-d824-4f82-9d63-eaa4ba04f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = yhat.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "de22dad8-0013-41ee-82d6-d6af3f25d35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.50346}\n"
     ]
    }
   ],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "accuracy = metric.compute(predictions=predictions, references=references)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "85298751-b987-4403-a7cc-800cda5f2812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.6697351442672236}\n"
     ]
    }
   ],
   "source": [
    "metric = evaluate.load(\"f1\")\n",
    "f1 = metric.compute(predictions=predictions, references=references)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1320adb3-0575-4feb-9273-08206bf4b8bd",
   "metadata": {},
   "source": [
    "# Summary generation: BART"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9210638-948b-426b-aa79-2a2def78f36b",
   "metadata": {},
   "source": [
    "**Tokenization using BART Tokenizer**\n",
    "- 대규모 데이터셋을 다루거나 실시간 처리가 필요한 경우 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "521b5620-5399-4b54-ad22-a16d778f7e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6934391-ef02-4326-a12e-8e5a7a7a8720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0bcfa4c8784fbbb3d3387523f12bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13e4896b8214fbfa79dc0a217357be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/177k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71796e6c1084ec6b4fccbebd51476a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/4.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93081b35efe040c4b1d0fb6cd877b69c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488b956a51e743659137184a39f0ae10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/682k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e534716aef124077b4386d2019c7a08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'BartTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(\"gogamza/kobart-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aacb5b7c-e14d-4ffe-afff-2ce4456e2913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 265, 264, 281, 283, 415, 5, 5, 461, 416, 5, 5, 416, 473, 5, 461, 415, 5, 5, 415, 5, 5, 416, 5, 464, 461, 417, 473, 5, 416, 5, 5, 417, 473, 476, 414, 5, 370, 416, 475, 5, 461, 416, 480, 5, 417, 473, 365, 417, 473, 476, 415, 468, 361, 245, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "text = \"BART는 요약 모델을 학습하기에 적합하다.\"\n",
    "encoded = tokenizer(text)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4502adeb-7b17-4725-bf1c-47dba1bb2ce8",
   "metadata": {},
   "source": [
    "**Structure of BART model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a14caaf-6235-4585-8f30-69b95fc9dd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7e63b4b-9487-460c-af2e-1320cea68de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c508b3d9254d8a8ddbacb5109277f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/495M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(\"gogamza/kobart-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db942184-adcd-415b-bb28-21e0b4ba8c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main name =  model\n",
      "L shared\n",
      "L encoder\n",
      "| L embed_tokens\n",
      "| L embed_positions\n",
      "| L layers\n",
      "| | L 0\n",
      "| | L 1\n",
      "| | L 2\n",
      "| | L 3\n",
      "| | L 4\n",
      "| | L 5\n",
      "| L layernorm_embedding\n",
      "L decoder\n",
      "| L embed_tokens\n",
      "| L embed_positions\n",
      "| L layers\n",
      "| | L 0\n",
      "| | L 1\n",
      "| | L 2\n",
      "| | L 3\n",
      "| | L 4\n",
      "| | L 5\n",
      "| L layernorm_embedding\n",
      "Main name =  lm_head\n"
     ]
    }
   ],
   "source": [
    "for main_name, main_module in model.named_children():\n",
    "    print(\"Main name = \", main_name)\n",
    "    for sub_name, sub_module in main_module.named_children():\n",
    "        print(\"L\", sub_name)\n",
    "        for ssub_name, ssub_module in sub_module.named_children():\n",
    "            print(\"| L\", ssub_name)\n",
    "            for sssub_name, sssub_module in ssub_module.named_children():\n",
    "                print(\"| | L\", sssub_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb40c1d-cb8e-4a25-accd-755e04c1a277",
   "metadata": {},
   "source": [
    "**Tokenization of movie news summary dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4818f7fb-d64c-4a75-8c6e-ff920883575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe17a0fc-432e-4359-b1d8-050f665e1615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(example, tokenizer):\n",
    "    return tokenizer(\n",
    "        example[\"document\"],\n",
    "        text_target=example[\"summary\"],\n",
    "        truncation=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb55e436-4b6f-4d63-b5ba-e36c8b4a8fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gogamza/kobart-base-v2\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b969f25f-39ee-42a2-87b9-56bbbfd1d06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039d2aeed173430d8203d73fa56fd4d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/787 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c4ff26b23d4f2db0471cdf922c27fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/66.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb59d280f2d749e88c6a95001fb924fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.45M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd5630e9cb3e42778dfcf938358e96b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.17M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792246aa4b3e42938f209aed45d60378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/22194 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5e2b331de0430e84b507842ee5753d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6bc0529851e4bed961293ff537f226d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2740 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
      "        num_rows: 22194\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
      "        num_rows: 2466\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
      "        num_rows: 2740\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"daekeun-ml/naver-news-summarization-ko\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "550a868c-aad7-4c56-8de6-b8e58cf984ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length = model.config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3716eef8-6bc4-4202-85db-b4108e320a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba3e89c70534202ace620ce74a0a448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22194 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e901e8752a4e3ca9d09adc19e4695e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110dadaee3c0453da599beafab1a3885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2740 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_dataset = dataset.map(\n",
    "    lambda example: preprocess_data(example, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f104223-2d78-45dc-80aa-661c51515b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 22194\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2466\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2740\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47257bb0-2b39-493d-b1ce-f645dc08390c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 416, 476, 367, 417, 473, 5, 461, 416, 463, 5, 415, 370, 476, 414, 5, 370, 461, 416, 5, 370, 415, 363, 367, 415, 5, 476, 415, 5, 5, 461, 415, 367, 5, 416, 475, 5, 416, 5, 476, 416, 364, 5, 415, 5, 5, 461, 416, 475, 5, 415, 469, 5, 461, 416, 5, 478, 416, 473, 465, 416, 5, 5, 461, 15684, 250, 416, 474, 5, 461, 415, 468, 367, 415, 479, 367, 461, 416, 480, 5, 416, 5, 5, 415, 362, 5, 461, 414, 5, 370, 415, 358, 5, 417, 473, 478, 461, 414, 370, 5, 416, 5, 5, 415, 5, 370, 243, 461, 416, 480, 473, 415, 372, 5, 414, 370, 5, 461, 417, 473, 476, 415, 370, 476, 414, 5, 370, 416, 475, 5, 461, 416, 5, 370, 415, 363, 367, 461, 414, 5, 5, 416, 480, 478, 416, 5, 476, 461, 415, 5, 464, 417, 469, 5, 415, 5, 365, 416, 5, 5, 461, 416, 5, 476, 416, 372, 478, 461, 417, 5, 473, 415, 469, 5, 415, 362, 5, 461, 416, 478, 464, 417, 473, 5, 461, 416, 5, 5, 415, 480, 362, 416, 5, 464, 461, 414, 5, 370, 416, 5, 5, 416, 5, 5, 414, 5, 370, 415, 358, 478, 461, 414, 5, 370, 416, 480, 473, 417, 473, 478, 461, 414, 370, 5, 416, 5, 5, 415, 5, 370, 243, 461, 417, 5, 5, 417, 5, 5, 461, 416, 5, 476, 416, 372, 478, 461, 416, 361, 5, 416, 5, 469, 414, 5, 370, 416, 475, 465, 416, 5, 476, 461, 415, 367, 5, 415, 362, 476, 415, 5, 478, 461, 417, 473, 5, 416, 5, 469, 415, 362, 5, 461, 416, 478, 464, 417, 473, 5, 461, 415, 367, 5, 416, 475, 5, 414, 5, 5, 416, 478, 5, 461, 414, 373, 478, 415, 5, 5, 415, 362, 5, 461, 17148, 416, 358, 370, 461, 416, 477, 5, 461, 416, 5, 5, 416, 463, 5, 461, 417, 5, 473, 415, 469, 5, 417, 473, 476, 414, 5, 480, 461, 415, 367, 5, 415, 362, 476, 415, 5, 464, 461, 416, 364, 5, 416, 477, 5, 414, 5, 5, 461, 416, 5, 464, 416, 468, 478, 416, 464, 480, 415, 370, 473, 461, 417, 5, 367, 416, 5, 465, 461, 415, 472, 371, 416, 5, 464, 461, 416, 372, 5, 416, 364, 464, 417, 473, 476, 414, 5, 370, 415, 358, 478, 461, 417, 474, 5, 415, 468, 361, 245, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-04 06:45:19.078590: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-04 06:45:19.174466: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-04 06:45:19.178502: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-11-04 06:45:19.178513: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-11-04 06:45:19.197754: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-04 06:45:19.554253: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-11-04 06:45:19.554302: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-11-04 06:45:19.554308: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>올�<unk> �<unk>반�<unk>� �<unk>�리�<unk>��<unk><unk> �<unk>�<unk>�<unk>��<unk>�<unk><unk> �<unk>�<unk> �<unk>�악�<unk><unk> 103�<unk> 달러 �<unk>�<unk><unk>�<unk> �<unk>��<unk>한 �<unk>�<unk><unk>�<unk>�, 정�<unk>�<unk> 하반�<unk>��<unk> �<unk>�리 �<unk><unk>제�<unk>� �<unk>��<unk>�<unk>��<unk><unk> �<unk>�출 �<unk>��<unk>�<unk> 위�<unk> �<unk><unk>력�<unk>� �<unk>��<unk><unk>�<unk><unk>�<unk>�로 �<unk>�정한 �<unk>�<unk><unk>�<unk>�, �<unk><unk>�<unk><unk> �<unk>�출 �<unk>�<unk>��<unk>�업�<unk>� �<unk>류�<unk>� �<unk>�<unk>��<unk> 위�<unk> �<unk>�<unk>�<unk><unk>�<unk> 규�<unk><unk>�<unk> 40조 �<unk> �<unk><unk>�<unk> �<unk>��<unk>하�<unk>� �<unk>류�<unk>� �<unk>�<unk>�<unk><unk> �<unk>�시선박 �<unk>��<unk>� 등�<unk>� �<unk>진하�<unk>�로 �<unk>다.</s>\n"
     ]
    }
   ],
   "source": [
    "sample = processed_dataset[\"train\"][\"labels\"][0]\n",
    "print(sample)\n",
    "print(tokenizer.decode(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73871c8b-b160-4f6e-b326-c9b46ebdf712",
   "metadata": {},
   "source": [
    "**DataCollatorForSeq2Seq**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f715429a-ca1f-4126-937e-4cfadcfeb471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "945f3cc3-58e9-4790-8a58-ec591c10967d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([4, 1026])\n",
      "attention_mask: torch.Size([4, 1026])\n",
      "labels: torch.Size([4, 515])\n"
     ]
    }
   ],
   "source": [
    "seq2seq_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "seq2seq_dataloader = DataLoader(\n",
    "    processed_dataset[\"train\"],\n",
    "    collate_fn=seq2seq_collator,\n",
    "    batch_size=4,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "seq2seq_iterator = iter(seq2seq_dataloader)\n",
    "seq2seq_batch = next(seq2seq_iterator)\n",
    "\n",
    "for key, value in seq2seq_batch.items():\n",
    "    print(f\"{key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e402d-7806-4a47-82cb-a400b4a33b66",
   "metadata": {},
   "source": [
    "**Model train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78715121-fae9-48e9-a472-5f3daf8da019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d81ed2ca-dd1b-4ae4-8d00-be9b9816cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"text-summarization\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=1,\n",
    "    eval_steps=200,\n",
    "    logging_steps=200,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "362aecbf-a700-4784-9c9e-f1720b837344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 06:55:18] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 06:55:18] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 06:55:18] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 06:55:18] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 06:55:18] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 06:55:19] We saw that you have a Intel(R) Core(TM) i7-14700K but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 06:55:19] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-14700K\n",
      "[codecarbon INFO @ 06:55:19] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 06:55:19]   Platform system: Linux-6.8.0-47-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 06:55:19]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 06:55:19]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 06:55:19]   Available RAM : 62.506 GB\n",
      "[codecarbon INFO @ 06:55:19]   CPU count: 28\n",
      "[codecarbon INFO @ 06:55:19]   CPU model: Intel(R) Core(TM) i7-14700K\n",
      "[codecarbon INFO @ 06:55:19]   GPU count: 1\n",
      "[codecarbon INFO @ 06:55:19]   GPU model: 1 x NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=seq2seq_collator,\n",
    "    train_dataset=processed_dataset[\"train\"].select(range(10000)),\n",
    "    eval_dataset=processed_dataset[\"validation\"].select(range(100))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a9228ae-ee80-45f3-b780-567834dfe57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnoahyun1222\u001b[0m (\u001b[33mjiyun\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/user/kjy/DL-projects/hugging_face/003_NLP/wandb/run-20241104_065530-k3uqelpr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jiyun/huggingface/runs/k3uqelpr' target=\"_blank\">text-summarization</a></strong> to <a href='https://wandb.ai/jiyun/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jiyun/huggingface' target=\"_blank\">https://wandb.ai/jiyun/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jiyun/huggingface/runs/k3uqelpr' target=\"_blank\">https://wandb.ai/jiyun/huggingface/runs/k3uqelpr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 05:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.011900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.564600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.463600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.417200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.414700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 06:55:46] Energy consumed for RAM : 0.000098 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 06:55:46] Energy consumed for all GPUs : 0.001516 kWh. Total GPU Power : 362.8189922798815 W\n",
      "[codecarbon INFO @ 06:55:46] Energy consumed for all CPUs : 0.000178 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 06:55:46] 0.001792 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 06:56:01] Energy consumed for RAM : 0.000196 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 06:56:01] Energy consumed for all GPUs : 0.003063 kWh. Total GPU Power : 371.4208170299897 W\n",
      "[codecarbon INFO @ 06:56:01] Energy consumed for all CPUs : 0.000355 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 06:56:01] 0.003613 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 06:56:16] Energy consumed for RAM : 0.000293 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 06:56:16] Energy consumed for all GPUs : 0.004634 kWh. Total GPU Power : 376.9719830244848 W\n",
      "[codecarbon INFO @ 06:56:16] Energy consumed for all CPUs : 0.000532 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 06:56:16] 0.005459 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 06:56:31] Energy consumed for RAM : 0.000391 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 06:56:31] Energy consumed for all GPUs : 0.006188 kWh. Total GPU Power : 373.0034419315131 W\n",
      "[codecarbon INFO @ 06:56:31] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 06:56:31] 0.007287 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 06:56:46] Energy consumed for RAM : 0.000489 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 06:56:46] Energy consumed for all GPUs : 0.007752 kWh. Total GPU Power : 375.5888991752936 W\n",
      "[codecarbon INFO @ 06:56:46] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 06:56:46] 0.009127 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 06:57:01] Energy consumed for RAM : 0.000586 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 06:57:01] Energy consumed for all GPUs : 0.009337 kWh. Total GPU Power : 380.2238662038046 W\n",
      "[codecarbon INFO @ 06:57:01] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 06:57:01] 0.010986 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 06:57:16] Energy consumed for RAM : 0.000684 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 06:57:16] Energy consumed for all GPUs : 0.010907 kWh. Total GPU Power : 376.9176850143805 W\n",
      "[codecarbon INFO @ 06:57:16] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 06:57:16] 0.012831 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 06:57:31] Energy consumed for RAM : 0.000781 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 06:57:31] Energy consumed for all GPUs : 0.012489 kWh. Total GPU Power : 379.91321679981513 W\n",
      "[codecarbon INFO @ 06:57:31] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 06:57:31] 0.014688 kWh of electricity used since the beginning.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "[codecarbon INFO @ 06:57:46] Energy consumed for RAM : 0.000879 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 06:57:46] Energy consumed for all GPUs : 0.013986 kWh. Total GPU Power : 359.4025402149218 W\n",
      "[codecarbon INFO @ 06:57:46] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 06:57:46] 0.016460 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 06:58:01] Energy consumed for RAM : 0.000977 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 06:58:01] Energy consumed for all GPUs : 0.015576 kWh. Total GPU Power : 381.62091729818826 W\n",
      "[codecarbon INFO @ 06:58:01] Energy consumed for all CPUs : 0.001772 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 06:58:01] 0.018325 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 06:58:16] Energy consumed for RAM : 0.001074 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 06:58:16] Energy consumed for all GPUs : 0.017155 kWh. Total GPU Power : 379.51726684104085 W\n",
      "[codecarbon INFO @ 06:58:16] Energy consumed for all CPUs : 0.001948 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 06:58:16] 0.020177 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 06:58:31] Energy consumed for RAM : 0.001172 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 06:58:31] Energy consumed for all GPUs : 0.018752 kWh. Total GPU Power : 383.5110223390587 W\n",
      "[codecarbon INFO @ 06:58:31] Energy consumed for all CPUs : 0.002126 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 06:58:31] 0.022050 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 06:58:46] Energy consumed for RAM : 0.001269 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 06:58:46] Energy consumed for all GPUs : 0.020365 kWh. Total GPU Power : 387.152946150279 W\n",
      "[codecarbon INFO @ 06:58:46] Energy consumed for all CPUs : 0.002303 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 06:58:46] 0.023937 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 06:59:01] Energy consumed for RAM : 0.001367 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 06:59:01] Energy consumed for all GPUs : 0.021953 kWh. Total GPU Power : 381.3019052231072 W\n",
      "[codecarbon INFO @ 06:59:01] Energy consumed for all CPUs : 0.002480 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 06:59:01] 0.025801 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 06:59:16] Energy consumed for RAM : 0.001465 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 06:59:16] Energy consumed for all GPUs : 0.023536 kWh. Total GPU Power : 380.6997629428728 W\n",
      "[codecarbon INFO @ 06:59:16] Energy consumed for all CPUs : 0.002657 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 06:59:16] 0.027658 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 06:59:31] Energy consumed for RAM : 0.001562 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 06:59:31] Energy consumed for all GPUs : 0.025122 kWh. Total GPU Power : 380.8930735902486 W\n",
      "[codecarbon INFO @ 06:59:31] Energy consumed for all CPUs : 0.002834 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 06:59:31] 0.029518 kWh of electricity used since the beginning.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
      "[codecarbon INFO @ 06:59:46] Energy consumed for RAM : 0.001660 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 06:59:46] Energy consumed for all GPUs : 0.026625 kWh. Total GPU Power : 360.94413816678446 W\n",
      "[codecarbon INFO @ 06:59:46] Energy consumed for all CPUs : 0.003011 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 06:59:46] 0.031296 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:00:01] Energy consumed for RAM : 0.001757 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 07:00:01] Energy consumed for all GPUs : 0.028213 kWh. Total GPU Power : 381.2681090962898 W\n",
      "[codecarbon INFO @ 07:00:01] Energy consumed for all CPUs : 0.003188 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 07:00:01] 0.033159 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:00:16] Energy consumed for RAM : 0.001855 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 07:00:16] Energy consumed for all GPUs : 0.029816 kWh. Total GPU Power : 385.0718804728087 W\n",
      "[codecarbon INFO @ 07:00:16] Energy consumed for all CPUs : 0.003365 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 07:00:16] 0.035036 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:00:31] Energy consumed for RAM : 0.001953 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 07:00:31] Energy consumed for all GPUs : 0.031402 kWh. Total GPU Power : 380.7554563672564 W\n",
      "[codecarbon INFO @ 07:00:31] Energy consumed for all CPUs : 0.003542 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 07:00:31] 0.036897 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:00:46] Energy consumed for RAM : 0.002050 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 07:00:46] Energy consumed for all GPUs : 0.033000 kWh. Total GPU Power : 382.96553265878873 W\n",
      "[codecarbon INFO @ 07:00:46] Energy consumed for all CPUs : 0.003720 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 07:00:46] 0.038770 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:00:50] Energy consumed for RAM : 0.002072 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 07:00:50] Energy consumed for all GPUs : 0.033343 kWh. Total GPU Power : 377.2240164891469 W\n",
      "[codecarbon INFO @ 07:00:50] Energy consumed for all CPUs : 0.003759 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 07:00:50] 0.039174 kWh of electricity used since the beginning.\n",
      "/usr/local/lib/python3.10/dist-packages/codecarbon/output.py:168: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame.from_records([dict(data.values)])])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=0.7132517852783203, metrics={'train_runtime': 320.3591, 'train_samples_per_second': 31.215, 'train_steps_per_second': 3.902, 'total_flos': 6109273497600000.0, 'train_loss': 0.7132517852783203, 'epoch': 1.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ce535-005c-45a0-8f02-d04d5d29f7ee",
   "metadata": {},
   "source": [
    "**Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e157460a-7f30-4e32-8a90-8c6b59c4caec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d88e2bee-5fe8-40ac-8d59-321ca54789e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(30000, 768, padding_idx=3)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a0595c6-0176-4a3d-b51f-cf691b5f5014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1415 > 1026). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[\"test\"][0]\n",
    "document = sample[\"document\"]\n",
    "inputs = tokenizer(document, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a3993d-54ef-4ac6-921e-bccec5d6b1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=1026,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04339dfb-c8b8-4c68-b730-1933e38bce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"원문 :\", document)\n",
    "print(\"정답 요약문 :\", sample[\"summary\"])\n",
    "print(\"생성 요약문 :\", tokenizer.decode(outputs[0]. skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ceccc3-d6e2-4eef-b209-3d572e4ddf22",
   "metadata": {},
   "source": [
    "# Question Answering: RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d080308-18e9-4ac8-99d9-22dfbe585efd",
   "metadata": {},
   "source": [
    "**Tokenization machine reading dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "127d2203-dfd0-4641-bfb0-988c48d790f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import RobertaTokenizerFast, RobertaForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9434394-0083-4c83-b7dc-f604f2f9ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(example, tokenizer):\n",
    "    tokenized = tokenizer(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    start_index = example[\"answers\"][\"answer_start\"][0]\n",
    "    answer_text = example[\"answers\"][\"text\"][0]\n",
    "    answer_tokens = tokenizer.encode(answer_text, add_special_tokens=False)\n",
    "    answer_tokens_length = len(answer_tokens)\n",
    "\n",
    "    start_context_tokens_index = tokenized[\"input_ids\"].index(tokenizer.sep_token_id)\n",
    "    context_offset_mapping = tokenized[\"offset_mapping\"][start_context_tokens_index:]\n",
    "    tokenized[\"start_positions\"] = len(tokenized[\"input_ids\"])\n",
    "    tokenized[\"end_positions\"] = len(tokenized[\"input_ids\"])\n",
    "\n",
    "    for i, (start_offset, end_offset) in enumerate(context_offset_mapping):\n",
    "        if start_offset >= start_index:\n",
    "            tokenized[\"start_positions\"] = start_context_tokens_index + i\n",
    "            tokenized[\"end_positions\"] = tokenized[\"start_positions\"] + answer_tokens_length\n",
    "            break\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e51298b4-0faa-4fea-b24e-436280fc51a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizerFast'.\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"klue/roberta-base\"\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n",
    "model = RobertaForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "719cae8f-df9e-4ab2-8857-a54e79d1e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"klue\", \"mrc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abb032f2-f76f-4f61-88d1-2bf218fef5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '제주도 장마 시작 … 중부는 이달 말부터',\n",
       " 'context': '올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 이달 말께 장마가 시작될 전망이다.17일 기상청에 따르면 제주도 남쪽 먼바다에 있는 장마전선의 영향으로 이날 제주도 산간 및 내륙지역에 호우주의보가 내려지면서 곳곳에 100㎜에 육박하는 많은 비가 내렸다. 제주의 장마는 평년보다 2~3일, 지난해보다는 하루 일찍 시작됐다. 장마는 고온다습한 북태평양 기단과 한랭 습윤한 오호츠크해 기단이 만나 형성되는 장마전선에서 내리는 비를 뜻한다.장마전선은 18일 제주도 먼 남쪽 해상으로 내려갔다가 20일께 다시 북상해 전남 남해안까지 영향을 줄 것으로 보인다. 이에 따라 20~21일 남부지방에도 예년보다 사흘 정도 장마가 일찍 찾아올 전망이다. 그러나 장마전선을 밀어올리는 북태평양 고기압 세력이 약해 서울 등 중부지방은 평년보다 사나흘가량 늦은 이달 말부터 장마가 시작될 것이라는 게 기상청의 설명이다. 장마전선은 이후 한 달가량 한반도 중남부를 오르내리며 곳곳에 비를 뿌릴 전망이다. 최근 30년간 평균치에 따르면 중부지방의 장마 시작일은 6월24~25일이었으며 장마기간은 32일, 강수일수는 17.2일이었다.기상청은 올해 장마기간의 평균 강수량이 350~400㎜로 평년과 비슷하거나 적을 것으로 내다봤다. 브라질 월드컵 한국과 러시아의 경기가 열리는 18일 오전 서울은 대체로 구름이 많이 끼지만 비는 오지 않을 것으로 예상돼 거리 응원에는 지장이 없을 전망이다.',\n",
       " 'news_category': '종합',\n",
       " 'source': 'hankyung',\n",
       " 'guid': 'klue-mrc-v1_train_12759',\n",
       " 'is_impossible': False,\n",
       " 'question_type': 1,\n",
       " 'question': '북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?',\n",
       " 'answers': {'answer_start': [478, 478], 'text': ['한 달가량', '한 달']}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd66a2cf-9011-431e-a2dd-8d3a388a7335",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = dataset.filter(lambda x: not x[\"is_impossible\"])\n",
    "processed_dataset = processed_dataset.map(\n",
    "    lambda example: preprocess_data(example, tokenizer), batched=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8e0303c-04ba-4aea-a246-7fe3f90603bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers', 'input_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions'],\n",
       "        num_rows: 12037\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers', 'input_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions'],\n",
       "        num_rows: 4008\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96fe5838-f4f1-487b-9150-9cc77d48feea",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = processed_dataset.filter(\n",
    "    lambda x: x[\"start_positions\"] < tokenizer.model_max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36e27013-bfc4-42ec-8f36-4e2893111a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers', 'input_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions'],\n",
       "        num_rows: 11095\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers', 'input_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions'],\n",
       "        num_rows: 3703\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "702c6e37-5781-4c30-a45d-4e18a522403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = processed_dataset.filter(\n",
    "    lambda x: x[\"end_positions\"] < tokenizer.model_max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed73dfd8-abe8-4fe8-9669-bb0e088b5fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers'],\n",
      "        num_rows: 17554\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers'],\n",
      "        num_rows: 5841\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0667a846-b409-451d-b6e8-7167468737c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers', 'input_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions'],\n",
      "        num_rows: 11083\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers', 'input_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions'],\n",
      "        num_rows: 3696\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(processed_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c729033-8f79-4be7-a5eb-d43a61b58327",
   "metadata": {},
   "source": [
    "**Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b072d512-2487-455f-a454-ab6265e9c8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3a1c5fc-8bc0-42a7-8b3b-560c9948326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorWithPadding(tokenizer, padding=\"longest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42fde89f-35b7-448d-bf23-7223467e0348",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"question-answering\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=1,\n",
    "    eval_steps=250,\n",
    "    logging_steps=250,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f804bce-fbe1-4c4a-9b4b-dcb72f2ba18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 07:39:41] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 07:39:41] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 07:39:41] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 07:39:41] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 07:39:41] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 07:39:42] We saw that you have a Intel(R) Core(TM) i7-14700K but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 07:39:42] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-14700K\n",
      "[codecarbon INFO @ 07:39:42] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 07:39:42]   Platform system: Linux-6.8.0-47-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 07:39:42]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 07:39:42]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 07:39:42]   Available RAM : 62.506 GB\n",
      "[codecarbon INFO @ 07:39:42]   CPU count: 28\n",
      "[codecarbon INFO @ 07:39:42]   CPU model: Intel(R) Core(TM) i7-14700K\n",
      "[codecarbon INFO @ 07:39:42]   GPU count: 1\n",
      "[codecarbon INFO @ 07:39:42]   GPU model: 1 x NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    data_collator=collator,\n",
    "    train_dataset=processed_dataset[\"train\"].select(range(10000)),\n",
    "    eval_dataset=processed_dataset[\"validation\"].select(range(100))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d292bcb-0f9f-41d9-9790-1451041768e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnoahyun1222\u001b[0m (\u001b[33mjiyun\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/user/kjy/DL-projects/hugging_face/003_NLP/wandb/run-20241104_073955-lvjcn9sl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jiyun/huggingface/runs/lvjcn9sl' target=\"_blank\">question-answering</a></strong> to <a href='https://wandb.ai/jiyun/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jiyun/huggingface' target=\"_blank\">https://wandb.ai/jiyun/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jiyun/huggingface/runs/lvjcn9sl' target=\"_blank\">https://wandb.ai/jiyun/huggingface/runs/lvjcn9sl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 02:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.503800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.407800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.209400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.172400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 07:40:11] Energy consumed for RAM : 0.000098 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 07:40:11] Energy consumed for all GPUs : 0.001660 kWh. Total GPU Power : 398.1109883204044 W\n",
      "[codecarbon INFO @ 07:40:11] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 07:40:11] 0.001935 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:40:26] Energy consumed for RAM : 0.000195 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 07:40:26] Energy consumed for all GPUs : 0.003345 kWh. Total GPU Power : 404.392284568547 W\n",
      "[codecarbon INFO @ 07:40:26] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 07:40:26] 0.003894 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:40:41] Energy consumed for RAM : 0.000293 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 07:40:41] Energy consumed for all GPUs : 0.005043 kWh. Total GPU Power : 407.7449664876083 W\n",
      "[codecarbon INFO @ 07:40:41] Energy consumed for all CPUs : 0.000532 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 07:40:41] 0.005868 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:40:56] Energy consumed for RAM : 0.000391 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 07:40:56] Energy consumed for all GPUs : 0.006633 kWh. Total GPU Power : 381.88579728282656 W\n",
      "[codecarbon INFO @ 07:40:56] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 07:40:56] 0.007733 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:41:11] Energy consumed for RAM : 0.000488 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 07:41:11] Energy consumed for all GPUs : 0.008349 kWh. Total GPU Power : 412.7029677037976 W\n",
      "[codecarbon INFO @ 07:41:11] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 07:41:11] 0.009723 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:41:26] Energy consumed for RAM : 0.000586 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 07:41:26] Energy consumed for all GPUs : 0.010080 kWh. Total GPU Power : 414.96434700598525 W\n",
      "[codecarbon INFO @ 07:41:26] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 07:41:26] 0.011729 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:41:41] Energy consumed for RAM : 0.000683 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 07:41:41] Energy consumed for all GPUs : 0.011811 kWh. Total GPU Power : 415.7349851287348 W\n",
      "[codecarbon INFO @ 07:41:41] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 07:41:41] 0.013735 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:41:56] Energy consumed for RAM : 0.000781 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 07:41:56] Energy consumed for all GPUs : 0.013440 kWh. Total GPU Power : 390.9453820155386 W\n",
      "[codecarbon INFO @ 07:41:56] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 07:41:56] 0.015638 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:42:11] Energy consumed for RAM : 0.000879 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 07:42:11] Energy consumed for all GPUs : 0.015171 kWh. Total GPU Power : 415.67474057806584 W\n",
      "[codecarbon INFO @ 07:42:11] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 07:42:11] 0.017644 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 07:42:14] Energy consumed for RAM : 0.000897 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 07:42:14] Energy consumed for all GPUs : 0.015494 kWh. Total GPU Power : 422.0246626927942 W\n",
      "[codecarbon INFO @ 07:42:14] Energy consumed for all CPUs : 0.001627 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 07:42:14] 0.018018 kWh of electricity used since the beginning.\n",
      "/usr/local/lib/python3.10/dist-packages/codecarbon/output.py:168: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame.from_records([dict(data.values)])])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=1.535464208984375, metrics={'train_runtime': 139.8668, 'train_samples_per_second': 71.497, 'train_steps_per_second': 8.937, 'total_flos': 2611440614437824.0, 'train_loss': 1.535464208984375, 'epoch': 1.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01573e3-308b-47b8-84bd-080a7d37fc54",
   "metadata": {},
   "source": [
    "**Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "860dd90e-dced-4bd5-bd13-578d5dfc1420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bec0ec2-3f7a-485f-91eb-f3946cb84447",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "question = \"대한민국의 수도는 어디인가요?\"\n",
    "context = \"서울은 대한민국의 수도다.\"\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efc7e64c-45e0-47f1-bd11-b77db57fcf37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  4892,  2079,  4438,  2259,  4069,  2179, 18119,    35,     2,\n",
       "          3671,  2073,  4892,  2079,  4438,  2062,    18,     2]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b44165f-da8d-4e94-8b9c-37ec968960e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[ 0.1863, -3.9321, -5.7286, -3.9440, -5.6872, -4.8295, -5.5411, -5.0764,\n",
       "         -4.7644,  0.1864,  3.8483, -3.1025, -1.0339, -4.7686, -0.6544, -5.3247,\n",
       "          0.0333,  0.1865]], device='cuda:0'), end_logits=tensor([[-2.5434, -4.2888, -3.6859, -4.6698, -4.0784, -4.5356, -4.2747, -5.0802,\n",
       "         -4.9306, -2.5435,  0.6271,  4.4405, -2.2527, -1.4613, -2.6562, -0.5889,\n",
       "         -2.8601, -2.5436]], device='cuda:0'), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51b3f7c4-f078-4767-96e8-41dd09a15b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"start_logits\"].argmax(dim=-1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c914b45-1a31-4ba3-8d47-8457f5836ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서울\n"
     ]
    }
   ],
   "source": [
    "start_index = outputs[\"start_logits\"].argmax(dim=-1).item()\n",
    "end_index = outputs[\"end_logits\"].argmax(dim=-1).item()\n",
    "predicted_ids = inputs[\"input_ids\"][0][start_index: end_index]\n",
    "predicted_text = tokenizer.decode(predicted_ids)\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aef7df-953e-47a4-bca0-9990115e4739",
   "metadata": {},
   "source": [
    "**Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b74fced5-546d-4825-afbe-d18b01a708c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b198af6-7445-4487-b252-60c0806e427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluator(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98cb6a79-3c17-4d02-8fb9-1776d3cc3ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ce0ec6d7e54cfb9a1c71fa24e2d33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`squad_v2_format` parameter not provided to QuestionAnsweringEvaluator.compute(). Automatically inferred `squad_v2_format` as False.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7107fc7b6a84910a7071ae51579739f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7817552a6df340bd8b658d8bb790e1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = metric.compute(\n",
    "    model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=processed_dataset[\"validation\"].select(range(100)),\n",
    "    id_column=\"guid\",\n",
    "    question_column=\"question\",\n",
    "    context_column=\"context\",\n",
    "    label_column=\"answers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6fe865d-f113-4663-91c9-0072d42f498d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 3.0, 'f1': 25.43333333333334, 'total_time_in_seconds': 1.0021326879505068, 'samples_per_second': 99.78718507278029, 'latency_in_seconds': 0.010021326879505068}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8c0ee9-15c1-49ca-9244-55c621365d19",
   "metadata": {},
   "source": [
    "# Machine Translation: T5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c6ebf-6e35-47f3-b79a-7eed09b755f7",
   "metadata": {},
   "source": [
    "**Tokenization OPUS-100 dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e81ce7c6-f93f-480f-a15e-19cedaabad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71224580-a314-4c6a-9f5e-719b3bed583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(example, tokenizer):\n",
    "    translation = example[\"translation\"]\n",
    "    translation_source = [\"en: \" + instance[\"en\"] for instance in translation]\n",
    "    translation_target = [\"ko: \" + instance[\"ko\"] for instance in translation]\n",
    "    tokenized = tokenizer(\n",
    "        translation_source, text_target=translation_target, truncation=True\n",
    "    )\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a68aea22-c85b-43cd-afae-65adbcbfc40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e67f08333d74707945ef42f5c613c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e12604fda34c15bf33f2300aa46216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/1.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334ae0f7a89d4754abcad243d60fadeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/4.17M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d81895c6ac54d56a70a42d8846b3821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588695d834f94f0f9194ceb4f91fee56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/893 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type longt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07712e6fadff4c6d94eece0ae918f7e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/439M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at KETI-AIR/long-ke-t5-small and are newly initialized: ['encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"KETI-AIR/long-ke-t5-small\"\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dcf8f7bd-4022-4797-8f56-c0ebe51cfddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76e2898a3a8455ca6955d4f2fa4f914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/65.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a168f8f83fd04ca5b72c2cd7f28c7892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/143k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd3670050a0473d922ecef7aa4442e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/70.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ee616457ed45daaa0a1f075a93ff08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/144k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8897904b084bce9efbf4bcc4f11acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14a60d6bc6d4f3ab15debf534c02bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f860d5b06de644839ce60b0e2031f1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "320176be-677d-41ef-ae26-116ef7a76e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': \"They're shaped like a bus.\", 'ko': '할머니처럼 만들었지만.. ? 엉망이지만..'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "21c1cfb9-35bc-4d64-a3f2-d5d44b740523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2b097a3fdc4971a8a4512366b9c0f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f0098c405e4267a781d7dd64b141ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025c8fbe111f4b99ab02eae277930e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_dataset = dataset.map(\n",
    "    lambda example: preprocess_data(example, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "535b40ce-135a-42d2-9a59-634e986d83d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1000000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1516b89f-9e78-45f2-a910-6f7426785f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = processed_dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7fe67d19-1e23-438d-b4c7-b9d0bd408b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [20004, 20525, 20048, 20298, 20480, 20025, 20263, 20027, 20187, 20050, 43305, 20009, 21015, 20047, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [20004, 23477, 20048, 92, 14, 4256, 11, 1363, 71, 1133, 2951, 20371, 33, 16, 75, 242, 10, 513, 20047, 1]}\n",
      "변환된 출발 언어 :  en: What makes you think I want an intro to anyone?</s>\n",
      "변환된 도착 언어 :  ko: 내가 너를 누구에게 소개하고 싶어한다고 생각하니?</s>\n"
     ]
    }
   ],
   "source": [
    "print(sample)\n",
    "print(\"변환된 출발 언어 : \", tokenizer.decode(sample[\"input_ids\"]))\n",
    "print(\"변환된 도착 언어 : \", tokenizer.decode(sample[\"labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d3cde3-be4f-4a70-b7bf-7af779debb49",
   "metadata": {},
   "source": [
    "**Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc5eb370-f96c-400e-bebf-432edc5cf58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "230065be-47ec-4f1d-80c0-f07c661e1781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 00:17:55] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 00:17:55] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 00:17:55] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 00:17:55] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 00:17:55] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 00:17:56] We saw that you have a Intel(R) Core(TM) i7-14700K but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 00:17:56] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-14700K\n",
      "[codecarbon INFO @ 00:17:56] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 00:17:56]   Platform system: Linux-6.8.0-47-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 00:17:56]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 00:17:56]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 00:17:56]   Available RAM : 62.506 GB\n",
      "[codecarbon INFO @ 00:17:56]   CPU count: 28\n",
      "[codecarbon INFO @ 00:17:56]   CPU model: Intel(R) Core(TM) i7-14700K\n",
      "[codecarbon INFO @ 00:17:56]   GPU count: 1\n",
      "[codecarbon INFO @ 00:17:56]   GPU model: 1 x NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "seq2seq_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "training_arguments = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"t5-translation\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=1,\n",
    "    eval_steps=2500,\n",
    "    logging_steps=2500,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    data_collator=seq2seq_collator,\n",
    "    train_dataset=processed_dataset[\"train\"].select(range(100000)),\n",
    "    eval_dataset=processed_dataset[\"validation\"].select(range(1000))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "12370b5d-e62f-4187-bb9a-696c4dc8af2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12500' max='12500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12500/12500 16:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.127000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.870300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.817100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.772600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.741300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 00:18:21] Energy consumed for RAM : 0.000098 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:18:21] Energy consumed for all GPUs : 0.000525 kWh. Total GPU Power : 125.86140561234407 W\n",
      "[codecarbon INFO @ 00:18:21] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:18:21] 0.000799 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:18:36] Energy consumed for RAM : 0.000195 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:18:36] Energy consumed for all GPUs : 0.001057 kWh. Total GPU Power : 127.80701389098887 W\n",
      "[codecarbon INFO @ 00:18:36] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:18:36] 0.001607 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:18:51] Energy consumed for RAM : 0.000293 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:18:51] Energy consumed for all GPUs : 0.001563 kWh. Total GPU Power : 121.52024154130527 W\n",
      "[codecarbon INFO @ 00:18:51] Energy consumed for all CPUs : 0.000531 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:18:51] 0.002388 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:19:06] Energy consumed for RAM : 0.000391 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:19:06] Energy consumed for all GPUs : 0.002102 kWh. Total GPU Power : 129.2091375743584 W\n",
      "[codecarbon INFO @ 00:19:06] Energy consumed for all CPUs : 0.000708 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:19:06] 0.003201 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:19:21] Energy consumed for RAM : 0.000488 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:19:21] Energy consumed for all GPUs : 0.002636 kWh. Total GPU Power : 128.18517962749812 W\n",
      "[codecarbon INFO @ 00:19:21] Energy consumed for all CPUs : 0.000885 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:19:21] 0.004009 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:19:36] Energy consumed for RAM : 0.000586 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:19:36] Energy consumed for all GPUs : 0.003154 kWh. Total GPU Power : 124.31325503936145 W\n",
      "[codecarbon INFO @ 00:19:36] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:19:36] 0.004802 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:19:51] Energy consumed for RAM : 0.000684 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:19:51] Energy consumed for all GPUs : 0.003699 kWh. Total GPU Power : 130.979749835127 W\n",
      "[codecarbon INFO @ 00:19:51] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:19:51] 0.005623 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:20:06] Energy consumed for RAM : 0.000781 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:20:06] Energy consumed for all GPUs : 0.004205 kWh. Total GPU Power : 121.434802858931 W\n",
      "[codecarbon INFO @ 00:20:06] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:20:06] 0.006403 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:20:21] Energy consumed for RAM : 0.000879 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:20:21] Energy consumed for all GPUs : 0.004746 kWh. Total GPU Power : 129.77163204831842 W\n",
      "[codecarbon INFO @ 00:20:21] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:20:21] 0.007219 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:20:36] Energy consumed for RAM : 0.000977 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:20:36] Energy consumed for all GPUs : 0.005280 kWh. Total GPU Power : 128.01695705819327 W\n",
      "[codecarbon INFO @ 00:20:36] Energy consumed for all CPUs : 0.001771 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:20:36] 0.008027 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:20:51] Energy consumed for RAM : 0.001074 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:20:51] Energy consumed for all GPUs : 0.005792 kWh. Total GPU Power : 123.03071256839954 W\n",
      "[codecarbon INFO @ 00:20:51] Energy consumed for all CPUs : 0.001948 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:20:51] 0.008815 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:21:06] Energy consumed for RAM : 0.001172 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:21:06] Energy consumed for all GPUs : 0.006328 kWh. Total GPU Power : 128.4964473582352 W\n",
      "[codecarbon INFO @ 00:21:06] Energy consumed for all CPUs : 0.002125 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:21:06] 0.009625 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:21:21] Energy consumed for RAM : 0.001270 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:21:21] Energy consumed for all GPUs : 0.006847 kWh. Total GPU Power : 124.54709712493069 W\n",
      "[codecarbon INFO @ 00:21:21] Energy consumed for all CPUs : 0.002302 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:21:21] 0.010419 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:21:36] Energy consumed for RAM : 0.001367 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:21:36] Energy consumed for all GPUs : 0.007385 kWh. Total GPU Power : 129.3056735098768 W\n",
      "[codecarbon INFO @ 00:21:36] Energy consumed for all CPUs : 0.002479 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:21:36] 0.011232 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:21:51] Energy consumed for RAM : 0.001465 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:21:51] Energy consumed for all GPUs : 0.007918 kWh. Total GPU Power : 127.96159659614517 W\n",
      "[codecarbon INFO @ 00:21:51] Energy consumed for all CPUs : 0.002657 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:21:51] 0.012040 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:22:06] Energy consumed for RAM : 0.001563 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:22:06] Energy consumed for all GPUs : 0.008428 kWh. Total GPU Power : 122.40540924347333 W\n",
      "[codecarbon INFO @ 00:22:06] Energy consumed for all CPUs : 0.002834 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:22:06] 0.012824 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:22:21] Energy consumed for RAM : 0.001660 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:22:21] Energy consumed for all GPUs : 0.008963 kWh. Total GPU Power : 128.35153210569322 W\n",
      "[codecarbon INFO @ 00:22:21] Energy consumed for all CPUs : 0.003011 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:22:21] 0.013634 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:22:36] Energy consumed for RAM : 0.001758 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:22:36] Energy consumed for all GPUs : 0.009498 kWh. Total GPU Power : 128.39935399994403 W\n",
      "[codecarbon INFO @ 00:22:36] Energy consumed for all CPUs : 0.003188 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:22:36] 0.014443 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:22:51] Energy consumed for RAM : 0.001855 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:22:51] Energy consumed for all GPUs : 0.010012 kWh. Total GPU Power : 123.28093979375937 W\n",
      "[codecarbon INFO @ 00:22:51] Energy consumed for all CPUs : 0.003365 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:22:51] 0.015232 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:23:06] Energy consumed for RAM : 0.001953 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:23:06] Energy consumed for all GPUs : 0.010551 kWh. Total GPU Power : 129.58782430568826 W\n",
      "[codecarbon INFO @ 00:23:06] Energy consumed for all CPUs : 0.003542 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:23:06] 0.016046 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:23:21] Energy consumed for RAM : 0.002051 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:23:21] Energy consumed for all GPUs : 0.011064 kWh. Total GPU Power : 123.12092112264745 W\n",
      "[codecarbon INFO @ 00:23:21] Energy consumed for all CPUs : 0.003719 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:23:21] 0.016834 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:23:36] Energy consumed for RAM : 0.002148 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:23:36] Energy consumed for all GPUs : 0.011597 kWh. Total GPU Power : 127.83752415799101 W\n",
      "[codecarbon INFO @ 00:23:36] Energy consumed for all CPUs : 0.003896 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:23:36] 0.017641 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:23:51] Energy consumed for RAM : 0.002246 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:23:51] Energy consumed for all GPUs : 0.012132 kWh. Total GPU Power : 128.5315594354456 W\n",
      "[codecarbon INFO @ 00:23:51] Energy consumed for all CPUs : 0.004073 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:23:51] 0.018452 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:24:06] Energy consumed for RAM : 0.002344 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:24:06] Energy consumed for all GPUs : 0.012649 kWh. Total GPU Power : 123.97996203386478 W\n",
      "[codecarbon INFO @ 00:24:06] Energy consumed for all CPUs : 0.004250 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:24:06] 0.019243 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:24:21] Energy consumed for RAM : 0.002441 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:24:21] Energy consumed for all GPUs : 0.013183 kWh. Total GPU Power : 128.2321029316178 W\n",
      "[codecarbon INFO @ 00:24:21] Energy consumed for all CPUs : 0.004427 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:24:21] 0.020052 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:24:36] Energy consumed for RAM : 0.002539 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:24:36] Energy consumed for all GPUs : 0.013694 kWh. Total GPU Power : 122.5693767375657 W\n",
      "[codecarbon INFO @ 00:24:36] Energy consumed for all CPUs : 0.004604 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:24:36] 0.020837 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:24:51] Energy consumed for RAM : 0.002637 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:24:51] Energy consumed for all GPUs : 0.014234 kWh. Total GPU Power : 129.49907505252787 W\n",
      "[codecarbon INFO @ 00:24:51] Energy consumed for all CPUs : 0.004782 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:24:51] 0.021652 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:25:06] Energy consumed for RAM : 0.002734 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:25:06] Energy consumed for all GPUs : 0.014769 kWh. Total GPU Power : 128.3450337522671 W\n",
      "[codecarbon INFO @ 00:25:06] Energy consumed for all CPUs : 0.004959 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:25:06] 0.022462 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:25:21] Energy consumed for RAM : 0.002832 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:25:21] Energy consumed for all GPUs : 0.015271 kWh. Total GPU Power : 120.6033340120868 W\n",
      "[codecarbon INFO @ 00:25:21] Energy consumed for all CPUs : 0.005136 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:25:21] 0.023239 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:25:36] Energy consumed for RAM : 0.002930 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:25:36] Energy consumed for all GPUs : 0.015794 kWh. Total GPU Power : 125.64198650004413 W\n",
      "[codecarbon INFO @ 00:25:36] Energy consumed for all CPUs : 0.005313 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:25:36] 0.024037 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:25:51] Energy consumed for RAM : 0.003027 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:25:51] Energy consumed for all GPUs : 0.016317 kWh. Total GPU Power : 125.42422764187889 W\n",
      "[codecarbon INFO @ 00:25:51] Energy consumed for all CPUs : 0.005490 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:25:51] 0.024834 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:26:06] Energy consumed for RAM : 0.003125 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:26:06] Energy consumed for all GPUs : 0.016822 kWh. Total GPU Power : 121.17736377675753 W\n",
      "[codecarbon INFO @ 00:26:06] Energy consumed for all CPUs : 0.005667 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:26:06] 0.025614 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:26:21] Energy consumed for RAM : 0.003223 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:26:21] Energy consumed for all GPUs : 0.017355 kWh. Total GPU Power : 128.0872344281035 W\n",
      "[codecarbon INFO @ 00:26:21] Energy consumed for all CPUs : 0.005844 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:26:21] 0.026422 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:26:36] Energy consumed for RAM : 0.003320 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:26:36] Energy consumed for all GPUs : 0.017877 kWh. Total GPU Power : 125.09031631126982 W\n",
      "[codecarbon INFO @ 00:26:36] Energy consumed for all CPUs : 0.006021 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:26:36] 0.027218 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:26:51] Energy consumed for RAM : 0.003418 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:26:51] Energy consumed for all GPUs : 0.018378 kWh. Total GPU Power : 120.44510697421751 W\n",
      "[codecarbon INFO @ 00:26:51] Energy consumed for all CPUs : 0.006198 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:26:51] 0.027994 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:27:06] Energy consumed for RAM : 0.003516 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:27:06] Energy consumed for all GPUs : 0.018906 kWh. Total GPU Power : 126.68121297792756 W\n",
      "[codecarbon INFO @ 00:27:06] Energy consumed for all CPUs : 0.006375 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:27:06] 0.028797 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:27:21] Energy consumed for RAM : 0.003613 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:27:21] Energy consumed for all GPUs : 0.019402 kWh. Total GPU Power : 118.97302437031468 W\n",
      "[codecarbon INFO @ 00:27:21] Energy consumed for all CPUs : 0.006552 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:27:21] 0.029567 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:27:36] Energy consumed for RAM : 0.003711 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:27:36] Energy consumed for all GPUs : 0.019922 kWh. Total GPU Power : 124.89466781161421 W\n",
      "[codecarbon INFO @ 00:27:36] Energy consumed for all CPUs : 0.006729 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:27:36] 0.030362 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:27:51] Energy consumed for RAM : 0.003809 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:27:51] Energy consumed for all GPUs : 0.020445 kWh. Total GPU Power : 125.56825221457521 W\n",
      "[codecarbon INFO @ 00:27:51] Energy consumed for all CPUs : 0.006907 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:27:51] 0.031161 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:28:06] Energy consumed for RAM : 0.003906 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:28:06] Energy consumed for all GPUs : 0.020947 kWh. Total GPU Power : 120.35791043282491 W\n",
      "[codecarbon INFO @ 00:28:06] Energy consumed for all CPUs : 0.007084 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:28:06] 0.031937 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:28:21] Energy consumed for RAM : 0.004004 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:28:21] Energy consumed for all GPUs : 0.021468 kWh. Total GPU Power : 125.10491871055946 W\n",
      "[codecarbon INFO @ 00:28:21] Energy consumed for all CPUs : 0.007261 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:28:21] 0.032733 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:28:36] Energy consumed for RAM : 0.004101 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:28:36] Energy consumed for all GPUs : 0.021988 kWh. Total GPU Power : 124.82627273752345 W\n",
      "[codecarbon INFO @ 00:28:36] Energy consumed for all CPUs : 0.007438 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:28:36] 0.033527 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:28:51] Energy consumed for RAM : 0.004199 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:28:51] Energy consumed for all GPUs : 0.022492 kWh. Total GPU Power : 120.89471879069629 W\n",
      "[codecarbon INFO @ 00:28:51] Energy consumed for all CPUs : 0.007615 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:28:51] 0.034306 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:29:06] Energy consumed for RAM : 0.004297 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:29:06] Energy consumed for all GPUs : 0.023012 kWh. Total GPU Power : 124.88010564379097 W\n",
      "[codecarbon INFO @ 00:29:06] Energy consumed for all CPUs : 0.007792 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:29:06] 0.035101 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:29:21] Energy consumed for RAM : 0.004394 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:29:21] Energy consumed for all GPUs : 0.023536 kWh. Total GPU Power : 125.78034135947335 W\n",
      "[codecarbon INFO @ 00:29:21] Energy consumed for all CPUs : 0.007970 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:29:21] 0.035900 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:29:36] Energy consumed for RAM : 0.004492 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:29:36] Energy consumed for all GPUs : 0.024037 kWh. Total GPU Power : 120.65757014544673 W\n",
      "[codecarbon INFO @ 00:29:36] Energy consumed for all CPUs : 0.008146 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:29:36] 0.036675 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:29:51] Energy consumed for RAM : 0.004589 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:29:51] Energy consumed for all GPUs : 0.024570 kWh. Total GPU Power : 127.84215411262878 W\n",
      "[codecarbon INFO @ 00:29:51] Energy consumed for all CPUs : 0.008323 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:29:51] 0.037483 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:30:06] Energy consumed for RAM : 0.004687 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:30:06] Energy consumed for all GPUs : 0.025107 kWh. Total GPU Power : 128.74896678021824 W\n",
      "[codecarbon INFO @ 00:30:06] Energy consumed for all CPUs : 0.008501 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:30:06] 0.038295 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:30:21] Energy consumed for RAM : 0.004785 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:30:21] Energy consumed for all GPUs : 0.025623 kWh. Total GPU Power : 124.11969974937581 W\n",
      "[codecarbon INFO @ 00:30:21] Energy consumed for all CPUs : 0.008678 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:30:21] 0.039085 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:30:36] Energy consumed for RAM : 0.004882 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:30:36] Energy consumed for all GPUs : 0.026167 kWh. Total GPU Power : 130.52127877003008 W\n",
      "[codecarbon INFO @ 00:30:36] Energy consumed for all CPUs : 0.008855 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:30:36] 0.039904 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:30:51] Energy consumed for RAM : 0.004980 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:30:51] Energy consumed for all GPUs : 0.026673 kWh. Total GPU Power : 121.40380194307959 W\n",
      "[codecarbon INFO @ 00:30:51] Energy consumed for all CPUs : 0.009032 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:30:51] 0.040684 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:31:06] Energy consumed for RAM : 0.005078 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:31:06] Energy consumed for all GPUs : 0.027196 kWh. Total GPU Power : 125.55290561985494 W\n",
      "[codecarbon INFO @ 00:31:06] Energy consumed for all CPUs : 0.009209 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:31:06] 0.041482 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:31:21] Energy consumed for RAM : 0.005175 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:31:21] Energy consumed for all GPUs : 0.027720 kWh. Total GPU Power : 125.88854879028635 W\n",
      "[codecarbon INFO @ 00:31:21] Energy consumed for all CPUs : 0.009386 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:31:21] 0.042281 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:31:36] Energy consumed for RAM : 0.005273 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:31:36] Energy consumed for all GPUs : 0.028218 kWh. Total GPU Power : 119.58739211395935 W\n",
      "[codecarbon INFO @ 00:31:36] Energy consumed for all CPUs : 0.009563 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:31:36] 0.043054 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:31:51] Energy consumed for RAM : 0.005371 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:31:51] Energy consumed for all GPUs : 0.028733 kWh. Total GPU Power : 123.627517302352 W\n",
      "[codecarbon INFO @ 00:31:51] Energy consumed for all CPUs : 0.009740 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:31:51] 0.043844 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:32:06] Energy consumed for RAM : 0.005468 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:32:06] Energy consumed for all GPUs : 0.029253 kWh. Total GPU Power : 124.71626347008434 W\n",
      "[codecarbon INFO @ 00:32:06] Energy consumed for all CPUs : 0.009917 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:32:06] 0.044638 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:32:21] Energy consumed for RAM : 0.005566 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:32:21] Energy consumed for all GPUs : 0.029753 kWh. Total GPU Power : 119.92379319060545 W\n",
      "[codecarbon INFO @ 00:32:21] Energy consumed for all CPUs : 0.010094 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:32:21] 0.045413 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:32:36] Energy consumed for RAM : 0.005663 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:32:36] Energy consumed for all GPUs : 0.030280 kWh. Total GPU Power : 126.70259335004161 W\n",
      "[codecarbon INFO @ 00:32:36] Energy consumed for all CPUs : 0.010271 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:32:36] 0.046215 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:32:51] Energy consumed for RAM : 0.005761 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:32:51] Energy consumed for all GPUs : 0.030780 kWh. Total GPU Power : 119.90084838732577 W\n",
      "[codecarbon INFO @ 00:32:51] Energy consumed for all CPUs : 0.010449 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:32:51] 0.046990 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:33:06] Energy consumed for RAM : 0.005859 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:33:06] Energy consumed for all GPUs : 0.031301 kWh. Total GPU Power : 125.31493329501511 W\n",
      "[codecarbon INFO @ 00:33:06] Energy consumed for all CPUs : 0.010625 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:33:06] 0.047785 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:33:21] Energy consumed for RAM : 0.005956 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:33:21] Energy consumed for all GPUs : 0.031830 kWh. Total GPU Power : 126.99573286002439 W\n",
      "[codecarbon INFO @ 00:33:21] Energy consumed for all CPUs : 0.010803 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:33:21] 0.048589 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:33:36] Energy consumed for RAM : 0.006054 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:33:36] Energy consumed for all GPUs : 0.032329 kWh. Total GPU Power : 119.7709290691596 W\n",
      "[codecarbon INFO @ 00:33:36] Energy consumed for all CPUs : 0.010980 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:33:36] 0.049363 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:33:51] Energy consumed for RAM : 0.006152 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:33:51] Energy consumed for all GPUs : 0.032852 kWh. Total GPU Power : 125.53025452494559 W\n",
      "[codecarbon INFO @ 00:33:51] Energy consumed for all CPUs : 0.011157 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:33:51] 0.050161 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:34:06] Energy consumed for RAM : 0.006249 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:34:06] Energy consumed for all GPUs : 0.033375 kWh. Total GPU Power : 125.42534406681635 W\n",
      "[codecarbon INFO @ 00:34:06] Energy consumed for all CPUs : 0.011334 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:34:06] 0.050958 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:34:21] Energy consumed for RAM : 0.006347 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:34:21] Energy consumed for all GPUs : 0.033945 kWh. Total GPU Power : 136.8465370909323 W\n",
      "[codecarbon INFO @ 00:34:21] Energy consumed for all CPUs : 0.011511 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:34:21] 0.051803 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:34:36] Energy consumed for RAM : 0.006445 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:34:36] Energy consumed for all GPUs : 0.034521 kWh. Total GPU Power : 138.30602950519926 W\n",
      "[codecarbon INFO @ 00:34:36] Energy consumed for all CPUs : 0.011688 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:34:36] 0.052654 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 00:34:45] Energy consumed for RAM : 0.006502 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 00:34:45] Energy consumed for all GPUs : 0.034810 kWh. Total GPU Power : 117.45582649024746 W\n",
      "[codecarbon INFO @ 00:34:45] Energy consumed for all CPUs : 0.011792 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 00:34:45] 0.053105 kWh of electricity used since the beginning.\n",
      "/usr/local/lib/python3.10/dist-packages/codecarbon/output.py:168: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame.from_records([dict(data.values)])])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12500, training_loss=2.8656383984375, metrics={'train_runtime': 998.9481, 'train_samples_per_second': 100.105, 'train_steps_per_second': 12.513, 'total_flos': 1434328128700416.0, 'train_loss': 2.8656383984375, 'epoch': 1.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6be69b6-8a54-446e-81f1-598b61170f72",
   "metadata": {},
   "source": [
    "# Text Generation: LLaMA-3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297792de-40ba-444d-87e3-ce2e1425d572",
   "metadata": {},
   "source": [
    "**Load LLaMA-3.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33686a0f-a396-42fe-830f-88d61a0b3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "871ccb3a-6924-442c-bef9-b8327d3ef966",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f33775f1-fc84-4078-a112-9668bda7f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca8cc9e0-93b5-4598-beba-1dc05d10dda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "495a09c7-8ef2-4a64-bd96-5762d035bc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa268254-4412-4a1f-923e-8d56b624636f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 01:04:30.995537: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-05 01:04:31.095157: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-05 01:04:31.099341: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-11-05 01:04:31.099351: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-11-05 01:04:31.120858: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-05 01:04:31.586085: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-11-05 01:04:31.586135: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-11-05 01:04:31.586141: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0bd06da80d84ea295142371e555e928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383976e41ecb49d0bd3b8fa6b5e52124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510f69bcbbde487c8ac3090b1f7729ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50e842e32ce446597e87c29b6546f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d20f84fecc4c28b7b8cef4caf7f6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86e9af53a27437ab33b0c4e0b644627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a903630e8640a49dd076abcaba0273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd82e4438264f1589425e26a636e225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map={\"\": 0},\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08603eb0-f3e6-4f54-ad2a-ef6e75638678",
   "metadata": {},
   "source": [
    "**Communicate with LLaMA-3.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1eb31d0-b99d-44a5-b7f1-50c4e9710578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요. 무엇을 도와드릴까요?\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"안녕하세요.\"}\n",
    "]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4834d9-6dbc-49d2-bd4e-b767dd3bb855",
   "metadata": {},
   "source": [
    "**Settings LLaMA-3.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dc03e95-db4e-42a1-a004-0ea0ac8faaee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b98d6e70713438db048083add8a86d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2750522dbd492e9fee87e98e69cefd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/3.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d56b9f0218476ab59ba03d5122355f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4655c6986ea946978931cf50ba3c5e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "위키북스의 대표 저자를 알려주세요.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "윤대희, 김동화, 송종민, 진현두<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"s076923/llama3-wikibook-ko\")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "token = \"\"\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    token=token\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map={\"\": 0},\n",
    "    token=token\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][\"text\"][7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55ff127-2318-4803-88b5-1050b7d83441",
   "metadata": {},
   "source": [
    "**LoRA 설정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87122122-d939-4337-89b7-a375f8209833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b868dc8-b3db-42e9-bd4c-68224083c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=128,\n",
    "    lora_alpha=4,\n",
    "    lora_dropout=0.1,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f582723f-b0f8-4da0-995f-50d7b0c8f443",
   "metadata": {},
   "source": [
    "**Fine-tuning using STF Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e585c716-5233-46e9-a601-91de984007e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb0bc8a0-588a-4550-ba9e-fc4aa8b11a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"LLaMa-3.1\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=5,\n",
    "    learning_rate=2e-4,\n",
    "    max_steps=500,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=100,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7eae3c7-70c9-4d1e-befd-08dec50cd710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95f67a3dd224dbdb77aa96ede9ef37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "[codecarbon INFO @ 01:33:44] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 01:33:44] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 01:33:44] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 01:33:44] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 01:33:44] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 01:33:45] We saw that you have a Intel(R) Core(TM) i7-14700K but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 01:33:45] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-14700K\n",
      "[codecarbon INFO @ 01:33:45] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 01:33:45]   Platform system: Linux-6.8.0-47-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 01:33:45]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 01:33:45]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 01:33:45]   Available RAM : 62.506 GB\n",
      "[codecarbon INFO @ 01:33:45]   CPU count: 28\n",
      "[codecarbon INFO @ 01:33:45]   CPU model: Intel(R) Core(TM) i7-14700K\n",
      "[codecarbon INFO @ 01:33:45]   GPU count: 1\n",
      "[codecarbon INFO @ 01:33:45]   GPU model: 1 x NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    peft_config=peft_config,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68fd9a5c-532f-422f-9ae3-7b49d33c7e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnoahyun1222\u001b[0m (\u001b[33mjiyun\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/user/kjy/DL-projects/hugging_face/003_NLP/wandb/run-20241105_013356-4mlmtyw5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jiyun/huggingface/runs/4mlmtyw5' target=\"_blank\">LLaMa-3.1</a></strong> to <a href='https://wandb.ai/jiyun/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jiyun/huggingface' target=\"_blank\">https://wandb.ai/jiyun/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jiyun/huggingface/runs/4mlmtyw5' target=\"_blank\">https://wandb.ai/jiyun/huggingface/runs/4mlmtyw5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 01:46, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.214800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.266700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.189400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.128000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 01:34:12] Energy consumed for RAM : 0.000098 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 01:34:12] Energy consumed for all GPUs : 0.001088 kWh. Total GPU Power : 261.1869555144978 W\n",
      "[codecarbon INFO @ 01:34:12] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 01:34:12] 0.001363 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:34:27] Energy consumed for RAM : 0.000195 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 01:34:27] Energy consumed for all GPUs : 0.002198 kWh. Total GPU Power : 266.3891404550268 W\n",
      "[codecarbon INFO @ 01:34:27] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 01:34:27] 0.002748 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:34:42] Energy consumed for RAM : 0.000293 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 01:34:42] Energy consumed for all GPUs : 0.003282 kWh. Total GPU Power : 260.11814770468015 W\n",
      "[codecarbon INFO @ 01:34:42] Energy consumed for all CPUs : 0.000531 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 01:34:42] 0.004106 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:34:57] Energy consumed for RAM : 0.000391 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 01:34:57] Energy consumed for all GPUs : 0.004370 kWh. Total GPU Power : 261.2080174971729 W\n",
      "[codecarbon INFO @ 01:34:57] Energy consumed for all CPUs : 0.000708 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 01:34:57] 0.005469 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:35:12] Energy consumed for RAM : 0.000488 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 01:35:12] Energy consumed for all GPUs : 0.005461 kWh. Total GPU Power : 261.79425394306696 W\n",
      "[codecarbon INFO @ 01:35:12] Energy consumed for all CPUs : 0.000885 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 01:35:12] 0.006834 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:35:27] Energy consumed for RAM : 0.000586 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 01:35:27] Energy consumed for all GPUs : 0.006555 kWh. Total GPU Power : 262.66009032962637 W\n",
      "[codecarbon INFO @ 01:35:27] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 01:35:27] 0.008203 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:35:42] Energy consumed for RAM : 0.000684 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 01:35:42] Energy consumed for all GPUs : 0.007651 kWh. Total GPU Power : 263.0571541979825 W\n",
      "[codecarbon INFO @ 01:35:42] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 01:35:42] 0.009574 kWh of electricity used since the beginning.\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6729766e-49fc798f71414fcb0a298606;cb6c5b89-5d3a-4ea0-b634-b3315848c616)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[codecarbon INFO @ 01:35:43] Energy consumed for RAM : 0.000693 kWh. RAM Power : 23.439877510070804 W\n",
      "[codecarbon INFO @ 01:35:43] Energy consumed for all GPUs : 0.007727 kWh. Total GPU Power : 181.71246207597753 W\n",
      "[codecarbon INFO @ 01:35:43] Energy consumed for all CPUs : 0.001257 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 01:35:43] 0.009678 kWh of electricity used since the beginning.\n",
      "/usr/local/lib/python3.10/dist-packages/codecarbon/output.py:168: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame.from_records([dict(data.values)])])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.6607710571289063, metrics={'train_runtime': 108.3673, 'train_samples_per_second': 4.614, 'train_steps_per_second': 4.614, 'total_flos': 1161131615846400.0, 'train_loss': 0.6607710571289063, 'epoch': 50.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c76f34-0961-4e80-b22e-baba49a44643",
   "metadata": {},
   "source": [
    "**Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ecc809c-0c35-49ba-b67c-4dbe5773d02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "위를북스의 대표저자는 김동화, 송종민, 진현두입니다.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"위키북스의 대표 저자는 누구에요\"}\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        top_p=0.95,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd021de2-6527-4ca6-9ccc-1a12528fd694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 45])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0851787-ab5a-4a8d-a4f0-3a0a885287d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([66])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c216e03-f228-4db3-b834-e9ac6d9a282b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
