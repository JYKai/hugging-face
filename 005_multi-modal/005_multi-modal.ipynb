{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2930e53-ab6c-4c5e-8fe6-659e398e343e",
   "metadata": {},
   "source": [
    "# Image Captioning, Image-to-text : BLIP-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e448e108-82b0-4672-8c20-60db522b445d",
   "metadata": {},
   "source": [
    "## BLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55407ef5-660a-4f13-b2ed-88f390db2d49",
   "metadata": {},
   "source": [
    "**BLIP(Bootstrapping Language-Image Pre-training)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815b2937-f9a6-4fe2-94bd-76493f888d90",
   "metadata": {},
   "source": [
    "**Model settings about Blip2Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46361a32-64ce-43e1-963a-68afaaca86d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb652d92-f76b-4bdb-bd1a-7dc761494d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blip2Config {\n",
      "  \"architectures\": [\n",
      "    \"Blip2ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"image_text_hidden_size\": 256,\n",
      "  \"image_token_index\": null,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"model_type\": \"blip-2\",\n",
      "  \"num_query_tokens\": 32,\n",
      "  \"qformer_config\": {\n",
      "    \"classifier_dropout\": null,\n",
      "    \"model_type\": \"blip_2_qformer\"\n",
      "  },\n",
      "  \"text_config\": {\n",
      "    \"_name_or_path\": \"facebook/opt-2.7b\",\n",
      "    \"activation_dropout\": 0.0,\n",
      "    \"architectures\": [\n",
      "      \"OPTForCausalLM\"\n",
      "    ],\n",
      "    \"eos_token_id\": 50118,\n",
      "    \"ffn_dim\": 10240,\n",
      "    \"hidden_size\": 2560,\n",
      "    \"model_type\": \"opt\",\n",
      "    \"num_attention_heads\": 32,\n",
      "    \"num_hidden_layers\": 32,\n",
      "    \"prefix\": \"</s>\",\n",
      "    \"torch_dtype\": \"float16\",\n",
      "    \"word_embed_proj_dim\": 2560\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_decoder_only_language_model\": true,\n",
      "  \"vision_config\": {\n",
      "    \"dropout\": 0.0,\n",
      "    \"initializer_factor\": 1.0,\n",
      "    \"model_type\": \"blip_2_vision_model\",\n",
      "    \"num_channels\": 3,\n",
      "    \"projection_dim\": 512\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "\n",
    "config = Blip2Config.from_pretrained(model_name)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7163bde6-225d-41ed-bca2-0c13cb180455",
   "metadata": {},
   "source": [
    "**Model structure of Blip2ForConditionalGeneration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18075c6d-f18a-4111-b31a-960b73d35783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 00:33:20.855043: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-07 00:33:20.949538: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-07 00:33:20.953333: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-11-07 00:33:20.953343: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-11-07 00:33:20.977477: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-07 00:33:21.426052: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-11-07 00:33:21.426099: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-11-07 00:33:21.426105: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Blip2ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b53d1b1-f84e-4e6f-ace2-1f2d07bb1b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540e58b35fea4ce3963af5f4aa473376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    model_name, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9074eac1-484c-4cf9-b363-e66b9fe5e2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_model\n",
      "L embeddings\n",
      "| L patch_embedding\n",
      "L encoder\n",
      "| L layers\n",
      "|  L 0\n",
      "|  L 1\n",
      "|  L 2\n",
      "|  L 3\n",
      "|  L 4\n",
      "|  L 5\n",
      "|  L 6\n",
      "|  L 7\n",
      "|  L 8\n",
      "|  L 9\n",
      "|  L 10\n",
      "|  L 11\n",
      "|  L 12\n",
      "|  L 13\n",
      "|  L 14\n",
      "|  L 15\n",
      "|  L 16\n",
      "|  L 17\n",
      "|  L 18\n",
      "|  L 19\n",
      "|  L 20\n",
      "|  L 21\n",
      "|  L 22\n",
      "|  L 23\n",
      "|  L 24\n",
      "|  L 25\n",
      "|  L 26\n",
      "|  L 27\n",
      "|  L 28\n",
      "|  L 29\n",
      "|  L 30\n",
      "|  L 31\n",
      "|  L 32\n",
      "|  L 33\n",
      "|  L 34\n",
      "|  L 35\n",
      "|  L 36\n",
      "|  L 37\n",
      "|  L 38\n",
      "L post_layernorm\n",
      "qformer\n",
      "L layernorm\n",
      "L dropout\n",
      "L encoder\n",
      "| L layer\n",
      "|  L 0\n",
      "|  L 1\n",
      "|  L 2\n",
      "|  L 3\n",
      "|  L 4\n",
      "|  L 5\n",
      "|  L 6\n",
      "|  L 7\n",
      "|  L 8\n",
      "|  L 9\n",
      "|  L 10\n",
      "|  L 11\n",
      "language_projection\n",
      "language_model\n",
      "L model\n",
      "| L decoder\n",
      "|  L embed_tokens\n",
      "|  L embed_positions\n",
      "|  L final_layer_norm\n",
      "|  L layers\n",
      "L lm_head\n"
     ]
    }
   ],
   "source": [
    "for main_name, main_module in model.named_children():\n",
    "    print(main_name)\n",
    "    for sub_name, sub_module in main_module.named_children():\n",
    "        print(\"L\", sub_name)\n",
    "        for ssub_name, ssub_module in sub_module.named_children():\n",
    "            print(\"| L\", ssub_name)\n",
    "            for sssub_name, sssub_module in ssub_module.named_children():\n",
    "                print(\"|  L\", sssub_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe26cc-a352-4859-88df-58580ecad3a1",
   "metadata": {},
   "source": [
    "**BLIP-2 모델의 vision_model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "875161fc-9a1e-44c0-b3ff-de928049ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d25cde1b-b08e-4725-ac27-d0aeba8f7d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0311227e650a4eeeb23350b4d3eae20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(model_name)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    model_name, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff08cfa5-eae1-43da-aa10-f3aef14c0679",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "\n",
    "image = dataset[\"test\"][\"image\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18256121-100e-47d9-bf61-4ecfb6298b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=image, return_tensors=\"pt\").to(model.device, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f18a266-ebb5-4b36-8fbf-e087f1ae7c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeds = model.vision_model(\n",
    "    inputs[\"pixel_values\"], return_dict=True\n",
    ").last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a70c7ddc-ab39-4e04-85ee-b0df5e38fb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blip2VisionModel(\n",
      "  (embeddings): Blip2VisionEmbeddings(\n",
      "    (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
      "  )\n",
      "  (encoder): Blip2Encoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-38): 39 x Blip2EncoderLayer(\n",
      "        (self_attn): Blip2Attention(\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
      "          (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
      "        )\n",
      "        (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Blip2MLP(\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
      "          (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
      "        )\n",
      "        (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      ")\n",
      "tensor([[[ 0.1372,  0.1934,  0.5537,  ..., -0.2993, -0.2489,  0.2034],\n",
      "         [-0.2839, -0.4556,  0.7847,  ...,  0.2229,  0.3833, -0.4087],\n",
      "         [ 0.5356,  1.0469,  0.9648,  ...,  0.5571,  0.3008, -0.4507],\n",
      "         ...,\n",
      "         [ 0.0343, -0.0627,  0.3831,  ...,  0.2932, -0.3997, -0.1594],\n",
      "         [-1.0488,  0.7393,  0.5884,  ..., -0.6475, -0.9341,  0.0055],\n",
      "         [-1.1543,  1.0459,  0.7715,  ...,  0.5269, -0.4812, -0.8643]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([1, 257, 1408])\n"
     ]
    }
   ],
   "source": [
    "print(model.vision_model)\n",
    "print(image_embeds)\n",
    "print(image_embeds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44a7abc-f6a4-4ba7-ac02-0e4108169b25",
   "metadata": {},
   "source": [
    "**qformer module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c79f2c19-58f6-4081-8c40-267f3a9d71b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_attention_mask = torch.ones(\n",
    "    image_embeds.size()[:-1], dtype=torch.long, device=model.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05cf2a0c-791f-4141-86e0-d81bc6bae724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.query_tokens.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a77d87a3-28f8-492e-b7d8-f30f639b9e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tokens = model.query_tokens.expand(image_embeds.shape[0], -1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fc62455-faaa-432f-895f-ef0743de3a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tokens.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8913c6c9-ec9c-4ef1-8243-e0077ab92a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_outputs = model.qformer(\n",
    "    query_embeds=query_tokens,\n",
    "    encoder_hidden_states=image_embeds,\n",
    "    encoder_attention_mask=image_attention_mask,\n",
    "    return_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a349e4ee-674e-412f-9a7a-cac2785e924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_outputs = query_outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "617da2a8-2204-4418-95f8-c7b56f01dbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 257])\n",
      "torch.Size([1, 32, 768])\n",
      "torch.Size([1, 32, 768])\n"
     ]
    }
   ],
   "source": [
    "print(image_attention_mask.shape)\n",
    "print(query_tokens.shape)\n",
    "print(query_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1441b46-35c2-4f6e-af45-24249aab3fb3",
   "metadata": {},
   "source": [
    "**language_model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78f2ce4c-8981-4db5-b9d7-90ce776da79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model_inputs = model.language_projection(query_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd9271e1-def6-42bc-91be-8e0cf45b7c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 2560])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model_inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6773d9cf-110d-4be3-9aa9-d80b1c768d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_attention_mask = torch.ones(\n",
    "    language_model_inputs.size()[:-1],\n",
    "    dtype=torch.long,\n",
    "    device=model.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03889459-eb82-4c1b-8843-b2746b3c0fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = (\n",
    "    torch.LongTensor([[model.config.text_config.bos_token_id]])\n",
    "    .repeat(inputs[\"pixel_values\"].shape[0], 1)\n",
    "    .to(model.device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06af67af-cf3b-449d-a751-162b5f0d8dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ca991cf-db27-44ec-b254-0d9a0c913fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = torch.ones_like(input_ids)\n",
    "attention_mask = torch.cat(\n",
    "    [language_attention_mask, attention_mask.to(model.device)], dim=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1a4aa4c-1f0b-4b3b-a5b5-88488cc1ad4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c9f69de-6c6c-47be-9377-7dea646c167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_embeds = model.get_input_embeddings()(input_ids)\n",
    "inputs_embeds = torch.cat(\n",
    "    [language_model_inputs, inputs_embeds.to(model.device)], dim=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43dd64ac-9c3c-4521-8b3f-303436aa8da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 33, 2560])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_embeds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7e97a41-f83a-4bf6-b0d6-29ed3eb4b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.language_model.generate(\n",
    "    inputs_embeds=inputs_embeds, attention_mask=attention_mask, max_length=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19c6212e-1ce6-4678-b77d-3edaa25d1e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7109, 10017, 11963,    15,    10, 16433, 50118]], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26e71726-3f86-4024-8ed3-3526876e9478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af494b97-1293-4a58-b582-aa42eb211889",
   "metadata": {},
   "source": [
    "**Image captioning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca992bf7-531a-430e-9b39-3d9a19853245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c135baf2-f6d7-4e2f-884d-cf471c857742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ed38041f9445a19e20703e99fa9caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(model_name)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    model_name, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7827cff-7e48-47cf-9798-94144ca0722a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image tokens in BLIP-2 should be done in processing. Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your BLIP-2 model. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,  7109, 10017, 11963,    15,    10, 16433, 50118]],\n",
      "       device='cuda:0')\n",
      "two cats laying on a couch\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(model.device, dtype=torch.float16)\n",
    "generated_ids = model.generate(**inputs, max_length=50)\n",
    "print(generated_ids)\n",
    "\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1cc769c7-13b6-4824-ba2f-bf68455edaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image tokens in BLIP-2 should be done in processing. Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your BLIP-2 model. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A couch\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Question: Describe the location of th image. Answer:\"\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(model.device, dtype=torch.float16)\n",
    "\n",
    "generated_ids = model.generate(**inputs)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779068a0-9b03-466c-8598-794a58912a57",
   "metadata": {},
   "source": [
    "# 문서 질의 응답: Layout:LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad6936d-b527-4626-990d-5e6f72c101cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
